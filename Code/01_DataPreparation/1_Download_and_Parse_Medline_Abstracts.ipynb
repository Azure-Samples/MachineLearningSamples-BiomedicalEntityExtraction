{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Parse MEDLINE Abstracts\n",
    "This Notebook describes the way you can download and parse the publically available Medline Abstracts. There are about 812 XML files that are available on the ftp server. Each XML file conatins about 30,000 Document Abstracts.\n",
    "<ul>\n",
    "<li> First we download the Medline XMLs from their FTP Server and store them in a local directory on the head node of the Spark Cluster </li>\n",
    "<li> Next we parse the XMLs using a publically available Medline Parser and store the parsed content in Tab separated files on the container associated with the spark cluster. </li>\n",
    "</ul>\n",
    "<br>Note: This Notebook is meant to be run on a Spark Cluster. If you are running it through a jupyter notebbok, make sure to use the PySpark Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Parser \n",
    "Download and install the pubmed_parser library into the spark cluster nodes. You can us the egg file available in the repo or  produce the .egg file by running<br>\n",
    "<b>python setup.py bdist_egg </b><br>\n",
    "in repository and add import for it. The egg file file can be read from the blob storage. Once you have the egg file ready you can put it in the container associated with your spark cluster.\n",
    "<br>\n",
    "\n",
    "#### Installing a additional packages on Spark Nodes\n",
    "To install additional packages you need to use script action from the azure portal. see <a href = \"https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-customize-cluster-linux\">this</a>  <br>\n",
    "Here's an example:\n",
    "<br> To install unidecode, you can use script action (on your Spark Cluster)\n",
    "<br>add the following lines to your script file (.sh)\n",
    "<br><b>#!/usr/bin/env bash\n",
    "<br>/usr/bin/anaconda/bin/conda install unidecode</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f8e9a01dd50>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import requests\n",
    "import lxml\n",
    "import pip\n",
    "import unidecode #use script action to install it\n",
    "import os\n",
    "\n",
    "#Specify the path of the egg file\n",
    "spark.sparkContext.addPyFile('wasb:///pubmed_parser-0.1-py2.7.egg')\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home_dir = os.getcwd()\n",
    "medline_dir = os.path.join(home_dir, 'ftp.nlm.nih.gov', 'nlmdata','.medleasebaseline','gz')\n",
    "os.listdir(home_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Download the files </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "home_dir = os.getcwd()\n",
    "medline_dir = os.path.join(home_dir, 'ftp.nlm.nih.gov', 'nlmdata','.medleasebaseline','gz')\n",
    "def download_xml_gz_files(num):    \n",
    "    print(\"Download MEDLINE for the first time\") \n",
    "    for i in range(num, num + 1):    \n",
    "        remote_filename = 'ftp://ftp.nlm.nih.gov/nlmdata/.medleasebaseline/gz/medline16n%04d.xml.gz' % i\n",
    "        print 'downloading %s .....' % remote_filename\n",
    "        local_filename = os.path.join(medline_dir, 'medline16n%04d.xml.gz' % i)\n",
    "        \n",
    "        #don't download the xml file if it was already downloaded\n",
    "        if not os.path.exists(local_filename):\n",
    "            subprocess.call(['wget', '-x', remote_filename, '-r'])                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Function to save files with text separated by the specified delimiter </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveDfToCsv(df, tsvOutputDir, sep, includeHeader):        \n",
    "    df.repartition(1).write.\\\n",
    "        format(\"com.databricks.spark.csv\").\\\n",
    "        option(\"header\", includeHeader).\\\n",
    "        option(\"delimiter\", sep).\\\n",
    "        save(tsvOutputDir,  mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Parse the XMLs and save them as a Tab separated File </b><br>\n",
    "There are a total of 812 XML files. It would take time for downloading that much data. Its advisable to do it in batches of 50.\n",
    "Downloading and parsing 1 file takes approximately 25-30 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download MEDLINE for the first time\n",
      "downloading ftp://ftp.nlm.nih.gov/nlmdata/.medleasebaseline/gz/medline16n0001.xml.gz .....\n",
      "processing medline16n0001.xml.gz .....\n",
      "Download MEDLINE for the first time\n",
      "downloading ftp://ftp.nlm.nih.gov/nlmdata/.medleasebaseline/gz/medline16n0002.xml.gz .....\n",
      "processing medline16n0002.xml.gz ....."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pubmed_parser as pp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row  \n",
    "from pyspark.sql.functions import regexp_replace          \n",
    "\n",
    "home_dir = os.getcwd()\n",
    "medline_dir = os.path.join(home_dir, 'ftp.nlm.nih.gov', 'nlmdata','.medleasebaseline','gz')\n",
    "\n",
    "#Change 813 to a smaller number if you want to test. Downloading and Parsing 1 file takes ~25-30 seconds. \n",
    "for i in range(1, 813): \n",
    "    download_xml_gz_files(i)\n",
    "    file_collection = [os.path.join(medline_dir,'medline16n%04d.xml.gz'%x)  \n",
    "                       for x in range(i, i+1)]\n",
    "    medline_files_rdd = sc.parallelize(file_collection, numSlices=6000)\n",
    "    for x in file_collection:\n",
    "        print 'processing %s .....' % os.path.basename(x)\n",
    "        dicts_out = pp.parse_medline_xml(x)\n",
    "        parse_results_rdd = medline_files_rdd.\\\n",
    "            flatMap(lambda x: [Row(file_name = os.path.basename(x), **publication_dict) \n",
    "                               for publication_dict in dicts_out])\n",
    "    \n",
    "    #convert RDD into dataframe\n",
    "    parse_results_df = parse_results_rdd.toDF()\n",
    "    \n",
    "    #Remove additional new line characters present in the Affiliations field\"\n",
    "    parse_results_df = parse_results_df.withColumn(\"affiliation\", regexp_replace(\"affiliation\", \"[^\\\\S]\", \" \"))\n",
    "    \n",
    "    tsvOutputDir = 'wasb:///medline_baseline/' + str(i)    \n",
    "    saveDfToCsv(parse_results_df, tsvOutputDir, \"\\t\", \"true\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "NLP_DL_EntityRecognition local",
   "language": "python",
   "name": "nlp_dl_entityrecognition_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
