{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Uni-directional LSTM using Keras with CNTK Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp \"Location of the vectors\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir BC2\n",
    "!cp \"Location of train.txt\" BC2\n",
    "!cp \"Location of test.txt\" BC2\n",
    "!cp \"Location of evaluation script\" BC2\n",
    "!chmod 777 BC2/evalBC2.pl\n",
    "!ls BC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Data_Preparation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Data_Preparation.py\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import cPickle as cpickle\n",
    "\n",
    "class Data_Preparation:\n",
    "\n",
    "    def __init__ (self, classes, seq_length, train_file=None, test_file=None, vector_size = 100):\n",
    "        \n",
    "        # Some constants\n",
    "        self.DEFAULT_N_CLASSES = classes\n",
    "        self.DEFAULT_N_FEATURES = vector_size\n",
    "        self.DEFAULT_MAX_SEQ_LENGTH = seq_length\n",
    "        \n",
    "        # Other stuff\n",
    "        self.wordvecs = None\n",
    "        self.word_to_ix_map = {}\n",
    "        self.n_features = 0\n",
    "        self.n_tag_classes = 0\n",
    "        self.n_sentences_all = 0\n",
    "        self.tag_vector_map = {}\n",
    "        \n",
    "        self.max_sentence_len_train = 0\n",
    "        self.max_sentence_len_test = 0\n",
    "        self.max_sentence_len = 0\n",
    "        \n",
    "        self.all_X_train = []\n",
    "        self.all_Y_train = []\n",
    "        self.all_X_test = []\n",
    "        self.all_Y_test = []\n",
    "        self.unk_words = []\n",
    "        \n",
    "        self.read_and_parse_data(train_file, test_file)\n",
    "            \n",
    "    def get_data (self):\n",
    "        return (self.all_X_train, self.all_Y_train, self.all_X_test, self.all_Y_test, self.wordvecs)\n",
    "    \n",
    "    def decode_prediction_sequence (self, pred_seq):\n",
    "        \n",
    "        pred_tags = []\n",
    "        for class_prs in pred_seq:\n",
    "            class_vec = np.zeros(self.DEFAULT_N_CLASSES, dtype=np.int32)\n",
    "            class_vec[np.argmax(class_prs)] = 1\n",
    "            if tuple(class_vec.tolist()) in self.tag_vector_map:\n",
    "                pred_tags.append(self.tag_vector_map[tuple(class_vec.tolist())])\n",
    "            else:\n",
    "                print tuple(class_vec.tolist())\n",
    "        return pred_tags\n",
    "    \n",
    "    def read_and_parse_data (self, train_file, test_file, skip_unknown_words = False):\n",
    "        \n",
    "        ###Load the Word2Vec Model###\n",
    "        print(\"Loading W2V model\")\n",
    "        W2V_model = cpickle.load(open(\"Word2Vec_Model.p\", \"rb\"))\n",
    "        \n",
    "        vocab = list(W2V_model.keys())\n",
    "        \n",
    "        self.word_to_ix_map = {}\n",
    "        self.wordvecs = []\n",
    "        \n",
    "        ###Create LookUp Table for words and their word vectors###\n",
    "        print(\"Creating LookUp table\")\n",
    "        for index, word in enumerate(vocab):\n",
    "            self.word_to_ix_map[word] = index\n",
    "            self.wordvecs.append(W2V_model[vocab[index]])\n",
    "        \n",
    "        self.wordvecs = np.array(self.wordvecs)\n",
    "        print(len(self.wordvecs))\n",
    "        self.n_features = len(self.wordvecs[0])\n",
    "        print(self.n_features)\n",
    "        \n",
    "        # Add a zero vector for the Paddings\n",
    "        self.wordvecs = np.vstack((self.wordvecs, np.zeros(self.DEFAULT_N_FEATURES)))\n",
    "        zero_vec_pos = self.wordvecs.shape[0] - 1\n",
    "        \n",
    "        ##########################  READ TRAINING DATA  ######################### \n",
    "        with open(train_file, 'r') as f_train:\n",
    "            \n",
    "            self.n_tag_classes = self.DEFAULT_N_CLASSES\n",
    "            self.tag_vector_map = {}    # For storing one hot vector notation for each Tag\n",
    "            tag_class_id = 0            # Used to put 1 in the one hot vector notation\n",
    "            raw_data_train = []\n",
    "            raw_words_train = []\n",
    "            raw_tags_train = []        \n",
    "\n",
    "            # Process all lines in the file\n",
    "            for line in f_train:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    raw_data_train.append( (tuple(raw_words_train), tuple(raw_tags_train)))\n",
    "                    raw_words_train = []\n",
    "                    raw_tags_train = []\n",
    "                    continue\n",
    "                \n",
    "                word, tag = line.split('\\t')\n",
    "                \n",
    "                raw_words_train.append(word)\n",
    "                raw_tags_train.append(tag)\n",
    "                \n",
    "                if tag not in self.tag_vector_map:\n",
    "                    one_hot_vec = np.zeros(self.DEFAULT_N_CLASSES, dtype=np.int32)\n",
    "                    one_hot_vec[tag_class_id] = 1\n",
    "                    self.tag_vector_map[tag] = tuple(one_hot_vec)\n",
    "                    self.tag_vector_map[tuple(one_hot_vec)] = tag\n",
    "                    tag_class_id += 1\n",
    "                    \n",
    "        print(\"raw_nd = \" + str(len(raw_data_train)))\n",
    "        \n",
    "        #Adding a None Tag\n",
    "        one_hot_vec = np.zeros(self.DEFAULT_N_CLASSES, dtype = np.int32)\n",
    "        one_hot_vec[tag_class_id] = 1\n",
    "        self.tag_vector_map['NONE'] = tuple(one_hot_vec)\n",
    "        self.tag_vector_map[tuple(one_hot_vec)] = 'NONE'\n",
    "        tag_class_id += 1\n",
    "        \n",
    "        self.n_sentences_all = len(raw_data_train)\n",
    "\n",
    "        # Find the maximum sequence length for Training data\n",
    "        self.max_sentence_len_train = 0\n",
    "        for seq in raw_data_train:\n",
    "            if len(seq[0]) > self.max_sentence_len_train:\n",
    "                self.max_sentence_len_train = len(seq[0])\n",
    "                \n",
    "                \n",
    "        ##########################  READ TEST DATA  ######################### \n",
    "        with open(test_file, 'r') as f_test:\n",
    "            \n",
    "            self.n_tag_classes = self.DEFAULT_N_CLASSES\n",
    "            tag_class_id = 0 \n",
    "            raw_data_test = []\n",
    "            raw_words_test = []\n",
    "            raw_tags_test = []        \n",
    "\n",
    "            # Process all lines in the file\n",
    "            for line in f_test:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    raw_data_test.append( (tuple(raw_words_test), tuple(raw_tags_test)))\n",
    "                    raw_words_test = []\n",
    "                    raw_tags_test = []\n",
    "                    continue\n",
    "                \n",
    "                word, tag = line.split('\\t') \n",
    "                \n",
    "                if tag not in self.tag_vector_map:\n",
    "                    print \"added\"\n",
    "                    one_hot_vec = np.zeros(self.DEFAULT_N_CLASSES, dtype=np.int32)\n",
    "                    one_hot_vec[tag_class_id] = 1\n",
    "                    self.tag_vector_map[tag] = tuple(one_hot_vec)\n",
    "                    self.tag_vector_map[tuple(one_hot_vec)] = tag\n",
    "                    tag_class_id += 1\n",
    "                \n",
    "                raw_words_test.append(word)\n",
    "                raw_tags_test.append(tag)\n",
    "                \n",
    "                                    \n",
    "        print(\"raw_nd = \" + str(len(raw_data_test)))\n",
    "        self.n_sentences_all = len(raw_data_test)\n",
    "\n",
    "        # Find the maximum sequence length for Test Data\n",
    "        self.max_sentence_len_test = 0\n",
    "        for seq in raw_data_test:\n",
    "            if len(seq[0]) > self.max_sentence_len_test:\n",
    "                self.max_sentence_len_test = len(seq[0])\n",
    "                \n",
    "        #Find the maximum sequence length in both training and Testing dataset\n",
    "        self.max_sentence_len = max(self.max_sentence_len_train, self.max_sentence_len_test)\n",
    "        \n",
    "        ############## Create Train Vectors################\n",
    "        self.all_X_train, self.all_Y_train = [], []\n",
    "        \n",
    "        self.unk_words = []\n",
    "        count = 0\n",
    "        for word_seq, tag_seq in raw_data_train:  \n",
    "            \n",
    "            elem_wordvecs, elem_tags = [], []            \n",
    "            for ix in range(len(word_seq)):\n",
    "                w = word_seq[ix]\n",
    "                t = tag_seq[ix]\n",
    "                w = w.lower()\n",
    "                if w in self.word_to_ix_map :\n",
    "                    count += 1\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[w])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "\n",
    "                elif \"UNK\" in self.word_to_ix_map :\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[\"UNK\"])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "                \n",
    "                else:\n",
    "                    w = \"UNK\"       \n",
    "                    new_wv = 2 * np.random.randn(self.DEFAULT_N_FEATURES) - 1 # sample from normal distribution\n",
    "                    norm_const = np.linalg.norm(new_wv)\n",
    "                    new_wv /= norm_const\n",
    "                    self.wordvecs = np.vstack((self.wordvecs, new_wv))\n",
    "                    self.word_to_ix_map[w] = self.wordvecs.shape[0] - 1\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[w])\n",
    "                    elem_tags.append(list(self.tag_vector_map[t]))\n",
    "\n",
    "            \n",
    "            # Pad the sequences for missing entries to make them all the same length\n",
    "            nil_X = zero_vec_pos\n",
    "            nil_Y = np.array(self.tag_vector_map['NONE'])\n",
    "            pad_length = self.max_sentence_len - len(elem_wordvecs)\n",
    "            self.all_X_train.append( ((pad_length)*[nil_X]) + elem_wordvecs)\n",
    "            self.all_Y_train.append( ((pad_length)*[nil_Y]) + elem_tags)\n",
    "\n",
    "        self.all_X_train = np.array(self.all_X_train)\n",
    "        self.all_Y_train = np.array(self.all_Y_train)\n",
    "        \n",
    "        ########################Create TEST Vectors##########################\n",
    "\n",
    "        self.all_X_test, self.all_Y_test = [], []\n",
    "        \n",
    "        for word_seq, tag_seq in raw_data_test:  \n",
    "            \n",
    "            elem_wordvecs, elem_tags = [], []            \n",
    "            for ix in range(len(word_seq)):\n",
    "                w = word_seq[ix]\n",
    "                t = tag_seq[ix]\n",
    "                w = w.lower()\n",
    "                if w in self.word_to_ix_map:\n",
    "                    count += 1\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[w])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "                    \n",
    "                elif \"UNK\" in self.word_to_ix_map :\n",
    "                    self.unk_words.append(w)\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[\"UNK\"])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "                    \n",
    "                else:\n",
    "                    self.unk_words.append(w)\n",
    "                    w = \"UNK\"\n",
    "                    self.word_to_ix_map[w] = self.wordvecs.shape[0] - 1\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[w])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "                \n",
    "            # Pad the sequences for missing entries to make them all the same length\n",
    "            nil_X = zero_vec_pos\n",
    "            nil_Y = np.array(self.tag_vector_map['NONE'])\n",
    "            pad_length = self.max_sentence_len - len(elem_wordvecs)\n",
    "            self.all_X_test.append( ((pad_length)*[nil_X]) + elem_wordvecs)\n",
    "            self.all_Y_test.append( ((pad_length)*[nil_Y]) + elem_tags)\n",
    "\n",
    "        self.all_X_test = np.array(self.all_X_test)\n",
    "        self.all_Y_test = np.array(self.all_Y_test)\n",
    "        \n",
    "        print(\"UNK WORD COUNT \" + str(len(self.unk_words)))\n",
    "        print(\"Found WORDS COUNT \" + str(count))\n",
    "        print(\"TOTAL WORDS \" + str(count+len(self.unk_words)))\n",
    "        \n",
    "        return (self.all_X_train, self.all_Y_train, self.all_X_test, self.all_Y_test, self.wordvecs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NER_Model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NER_Model.py\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers.core import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import keras.backend as K\n",
    "import Data_Preparation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "class NER_Model:\n",
    "\n",
    "    def __init__ (self, reader):\n",
    "        \n",
    "        self.reader = reader\n",
    "        self.model = None\n",
    "        self.all_X_train, self.all_Y_train, self.all_X_test, self.all_Y_test, self.wordvecs = reader.get_data()\n",
    "        self.train_X = self.all_X_train\n",
    "        self.train_Y = self.all_Y_train\n",
    "        \n",
    "        self.test_X = self.all_X_test\n",
    "        self.test_Y = self.all_Y_test\n",
    "        \n",
    "    def load (self, filepath):\n",
    "        self.model = load_model(filepath)\n",
    "        \n",
    "    def save (self, filepath):\n",
    "        self.model.save(filepath)\n",
    "\n",
    "    def print_summary (self):\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def train (self, test_split = 0.2, epochs = 1, batch = 50, dropout = 0.2, reg_alpha = 0.0, units = 150, layers = 1):\n",
    "        \n",
    "        self.train_X = self.all_X_train\n",
    "        self.train_Y = self.all_Y_train\n",
    "        \n",
    "        self.test_X = self.all_X_test\n",
    "        self.test_Y = self.all_Y_test\n",
    "\n",
    "        print(\"Data Shapes\")\n",
    "        print(self.train_X.shape)\n",
    "        print(self.train_Y.shape)\n",
    "        print(self.test_X.shape)\n",
    "        print(self.test_Y.shape)\n",
    "        \n",
    "        dropout = 0.2\n",
    "\n",
    "        self.model = Sequential()        \n",
    "        self.model.add(Embedding(self.wordvecs.shape[0], self.wordvecs.shape[1], input_length = self.train_X.shape[1], \\\n",
    "                                 weights = [self.wordvecs], trainable = False))\n",
    "        \n",
    "        self.model.add(LSTM(units, return_sequences = True))\n",
    "        \n",
    "        self.model.add(Dropout(dropout))\n",
    "        \n",
    "        if layers > 1:\n",
    "            self.model.add(LSTM(units, return_sequences=True))\n",
    "            self.model.add(Dropout(dropout))\n",
    "            \n",
    "        self.model.add(TimeDistributed(Dense(self.train_Y.shape[2], activation='softmax', W_regularizer=l2(reg_alpha), b_regularizer=l2(reg_alpha))))\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')#, metrics=['accuracy', self.precision, self.recall, self.f1score])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "\n",
    "        self.model.fit(self.train_X, self.train_Y, epochs = epochs, batch_size = batch)\n",
    "        \n",
    "    def evaluate_1(self):\n",
    "        target = open(\"Pubmed_Output.txt\", 'w')\n",
    "        predicted_tags= []\n",
    "        test_data_tags = []\n",
    "        ind = 0\n",
    "        for x,y in zip(self.test_X, self.test_Y):\n",
    "            tags = self.model.predict(np.array([x]), batch_size=1)[0]\n",
    "            pred_tags = self.reader.decode_prediction_sequence(tags)\n",
    "            test_tags = self.reader.decode_prediction_sequence(y)\n",
    "            ind += 1\n",
    "            ### To see Progress ###\n",
    "            if ind%500 == 0: \n",
    "                print(\"Sentence\" + str(ind))\n",
    "\n",
    "            pred_tag_wo_none = []\n",
    "            test_tags_wo_none = []\n",
    "            \n",
    "            for index, test_tag in enumerate(test_tags):\n",
    "                if test_tag != \"NONE\":\n",
    "                    test_tags_wo_none.append(test_tag)\n",
    "                    pred_tag_wo_none.append(pred_tags[index])\n",
    "            \n",
    "            for wo in pred_tag_wo_none:\n",
    "                target.write(str(wo))\n",
    "                target.write(\"\\n\")\n",
    "            target.write(\"\\n\")\n",
    "            \n",
    "            for i,j in zip(pred_tags, test_tags):\n",
    "                if i != \"NONE\" and j != \"NONE\":\n",
    "                    test_data_tags.append(j)\n",
    "                    predicted_tags.append(i)\n",
    "\n",
    "        target.close()\n",
    "        \n",
    "        predicted_tags = np.array(predicted_tags)\n",
    "        test_data_tags = np.array(test_data_tags)\n",
    "        print(classification_report(test_data_tags, predicted_tags))\n",
    "\n",
    "        simple_conf_matrix = confusion_matrix(test_data_tags,predicted_tags)\n",
    "        all_tags = sorted(list(set(test_data_tags)))\n",
    "        conf_matrix = pd.DataFrame(columns = all_tags, index = all_tags)\n",
    "        for x,y in zip(simple_conf_matrix, all_tags):\n",
    "            conf_matrix[y] = x\n",
    "        conf_matrix = conf_matrix.transpose()\n",
    "        \n",
    "        return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Data_Preparation import Data_Preparation\n",
    "from NER_Model import NER_Model\n",
    "import cPickle as cp\n",
    "\n",
    "TRAIN_FILEPATH = \"BC2//BC2_train.txt\"\n",
    "TEST_FILEPATH = \"BC2//BC2_test.txt\"\n",
    "\n",
    "vector_size = 50\n",
    "classes = 3 + 1\n",
    "seq_length = 213\n",
    "layer_arg = 2\n",
    "ep_arg = 5\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    print(\"\\n\\n\\nRunning on BIO-NLP data\\n\\n\\n\")\n",
    "        \n",
    "    # Read the data\n",
    "    print(\"Initializing data...\")\n",
    "    reader = Data_Preparation(classes, seq_length, TRAIN_FILEPATH, TEST_FILEPATH, vector_size)\n",
    "    \n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test, wordvecs = reader.get_data()\n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training model... epochs = {0}, layers = {1}\".format(ep_arg, layer_arg))\n",
    "    nermodel = NER_Model(reader)\n",
    "    nermodel.train(epochs=ep_arg, layers=layer_arg)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    confusion_matrix = nermodel.evaluate_1()\n",
    "    print confusion_matrix\n",
    "\n",
    "    print(\"Done.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"Pubmed_Output.txt\")\n",
    "file2 = open(\"BC2//BC2_test.txt\")\n",
    "target = open(\"BC2//eval2.txt\", \"w\")\n",
    "\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "for line in file1:\n",
    "    list1.append(line)\n",
    "    \n",
    "for line in file2:\n",
    "    list2.append(line)\n",
    "    \n",
    "for ind, line in enumerate(list2):\n",
    "    x = line.split(\"\\t\")\n",
    "    if len(x) == 1:\n",
    "        target.write(\"\\n\")\n",
    "    else:\n",
    "        target.write(x[0])\n",
    "        target.write(\"\\t\")\n",
    "        if list1[ind] == \"NONE\":\n",
    "            target.write(\"O\")\n",
    "        else:\n",
    "            target.write(list1[ind])\n",
    "    ind += 1\n",
    "\n",
    "file1.close()\n",
    "file2.close()\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Biomedical Entity Recognition Performance (Genaral)                                         \r\n",
      "                                                                                         number(recall/precision/f-score) \r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "|                  |          complete match         |       right boundary match      |       left boundary match       |\r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "|   GENE    (6325) | 3313 (52.38% / 57.58% / 54.86%) | 4351 (68.79% / 75.62% / 72.04%) | 4222 (66.75% / 73.38% / 69.91%) |\r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "|  [-ALL-]  (6325) | 3313 (52.38% / 57.58% / 54.86%) | 4351 (68.79% / 75.62% / 72.04%) | 4222 (66.75% / 73.38% / 69.91%) |\r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./BC2/evalBC2.pl BC2/eval2.txt BC2/BC2_test.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
