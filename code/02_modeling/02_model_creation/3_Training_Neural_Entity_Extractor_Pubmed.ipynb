{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Entity Detector using Pubmed Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the Embeddings from source location to destination location <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!cp \"Location of Word2Vec_Model.p\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR <br><br>\n",
    "#### Generate the Embedding Matrix from Parquet files on the Container linked to your Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storage_account_name='76f8577bf451dsvm'\n",
    "storage_account_key='5DPDh+p3Xbg9BfS9d/OSrtQ/Utrat1Rr/NRrGU+x3cRYPZYi6B92WEWUIkM28Z8cGRsRz0cuSGb2mjyBCB0QXg=='\n",
    "storage_container_name ='hackathon2017-container'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the Embedding Matrix by reading the Parquet Files\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import fastparquet\n",
    "import cPickle as cpickle\n",
    "timestart = datetime.datetime.now()\n",
    "\n",
    "block_blob_service = BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n",
    "\n",
    "#Specify the string to look for in blob names from your container\n",
    "path_to_embeddings = \"wasb:///Word2VecModels/w2vmodel_entiredataset_vs_50_ws_5_mc_1000_with_gzip_array/\"\n",
    "\n",
    "#Specify the path where to store the downloaded files\n",
    "file_path = \"./PQ/W2V/\" \n",
    "\n",
    "generator = block_blob_service.list_blobs(storage_container_name)\n",
    "for blob in generator:\n",
    "    if path_to_embeddings in blob.name:\n",
    "        filename = blob.name.split(\"/\")[-1]\n",
    "        block_blob_service.get_blob_to_path(storage_container_name, blob.name, file_path + filename)\n",
    "        \n",
    "\n",
    "files = os.listdir(file_path)\n",
    "vector_size = 50\n",
    "print \"Vector Size\", vector_size\n",
    "\n",
    "Word2Vec_Model = {}\n",
    "\n",
    "print(\"Reading PQ files\")\n",
    "for index, filename in enumerate(files):\n",
    "    if \"part\" in filename:\n",
    "        print(index)\n",
    "        pfile = fastparquet.ParquetFile(file_path + filename) \n",
    "        arr =  pfile.to_pandas().values\n",
    "        arr = list(arr)                 \n",
    "        for ind, vals in enumerate(arr):\n",
    "            vec = vals[-vector_size:]\n",
    "            vec = np.array(vec)\n",
    "            Word2Vec_Model[vals[0]] = vec.astype('float32')\n",
    "\n",
    "#save the embedding matrix into a pickle file\n",
    "cpickle.dump(Word2Vec_Model, open(\"Word2Vec_Model.p\", \"wb\")) #Specify the path where you want to store the Embeddings\n",
    "    \n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds() / 60, 2)\n",
    "print (\"Time taken to execute above cell: \" + str(timedelta) + \" mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the Training Data, Testing Data, Evaluation Script to destination location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir Drugs_and_Diseases\n",
    "!cp \"Location of train.txt\" Drugs_and_Diseases\n",
    "!cp \"Location of test.txt\" Drugs_and_Diseases\n",
    "!cp \"Location of evaluation script\" Drugs_and_Diseases\n",
    "!chmod 777 Drugs_and_Diseases/evalD_a_D.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Training and Testing Data in the correct format for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Data_Preparation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Data_Preparation.py\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import cPickle as cpickle\n",
    "\n",
    "class Data_Preparation:\n",
    "\n",
    "    def __init__ (self, classes, seq_length, train_file=None, test_file=None, vector_size = 100):\n",
    "        \n",
    "        # Some constants\n",
    "        self.DEFAULT_N_CLASSES = classes\n",
    "        self.DEFAULT_N_FEATURES = vector_size\n",
    "        self.DEFAULT_MAX_SEQ_LENGTH = seq_length\n",
    "        \n",
    "        # Other stuff\n",
    "        self.wordvecs = None\n",
    "        self.word_to_ix_map = {}\n",
    "        self.n_features = 0\n",
    "        self.n_tag_classes = 0\n",
    "        self.n_sentences_all = 0\n",
    "        self.tag_vector_map = {}\n",
    "        \n",
    "        self.max_sentence_len_train = 0\n",
    "        self.max_sentence_len_test = 0\n",
    "        self.max_sentence_len = 0\n",
    "        \n",
    "        self.all_X_train = []\n",
    "        self.all_Y_train = []\n",
    "        self.all_X_test = []\n",
    "        self.all_Y_test = []\n",
    "        self.unk_words = []\n",
    "        \n",
    "        self.read_and_parse_data(train_file, test_file)\n",
    "            \n",
    "    def get_data (self):\n",
    "        return (self.all_X_train, self.all_Y_train, self.all_X_test, self.all_Y_test, self.wordvecs)\n",
    "    \n",
    "    def decode_prediction_sequence (self, pred_seq):\n",
    "        \n",
    "        pred_tags = []\n",
    "        for class_prs in pred_seq:\n",
    "            class_vec = np.zeros(self.DEFAULT_N_CLASSES, dtype=np.int32)\n",
    "            class_vec[np.argmax(class_prs)] = 1\n",
    "            if tuple(class_vec.tolist()) in self.tag_vector_map:\n",
    "                pred_tags.append(self.tag_vector_map[tuple(class_vec.tolist())])\n",
    "            else:\n",
    "                print tuple(class_vec.tolist())\n",
    "        return pred_tags\n",
    "    \n",
    "    def read_and_parse_data (self, train_file, test_file, skip_unknown_words = False):\n",
    "        \n",
    "        ###Load the Word2Vec Model###\n",
    "        print(\"Loading W2V model\")\n",
    "        W2V_model = cpickle.load(open(\"Word2Vec_Model.p\", \"rb\"))\n",
    "        \n",
    "        vocab = list(W2V_model.keys())\n",
    "        \n",
    "        self.word_to_ix_map = {}\n",
    "        self.wordvecs = []\n",
    "        \n",
    "        ###Create LookUp Table for words and their word vectors###\n",
    "        print(\"Creating LookUp table\")\n",
    "        for index, word in enumerate(vocab):\n",
    "            self.word_to_ix_map[word] = index\n",
    "            self.wordvecs.append(W2V_model[vocab[index]])\n",
    "        \n",
    "        self.wordvecs = np.array(self.wordvecs)\n",
    "        print(len(self.wordvecs))\n",
    "        self.n_features = len(self.wordvecs[0])\n",
    "        print(self.n_features)\n",
    "        \n",
    "        # Add a zero vector for the Paddings\n",
    "        self.wordvecs = np.vstack((self.wordvecs, np.zeros(self.DEFAULT_N_FEATURES)))\n",
    "        zero_vec_pos = self.wordvecs.shape[0] - 1\n",
    "        \n",
    "        ##########################  READ TRAINING DATA  ######################### \n",
    "        with open(train_file, 'r') as f_train:\n",
    "            \n",
    "            self.n_tag_classes = self.DEFAULT_N_CLASSES\n",
    "            self.tag_vector_map = {}    # For storing one hot vector notation for each Tag\n",
    "            tag_class_id = 0            # Used to put 1 in the one hot vector notation\n",
    "            raw_data_train = []\n",
    "            raw_words_train = []\n",
    "            raw_tags_train = []        \n",
    "\n",
    "            # Process all lines in the file\n",
    "            for line in f_train:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    raw_data_train.append( (tuple(raw_words_train), tuple(raw_tags_train)))\n",
    "                    raw_words_train = []\n",
    "                    raw_tags_train = []\n",
    "                    continue\n",
    "                \n",
    "                word, tag = line.split('\\t')\n",
    "                \n",
    "                raw_words_train.append(word)\n",
    "                raw_tags_train.append(tag)\n",
    "                \n",
    "                if tag not in self.tag_vector_map:\n",
    "                    one_hot_vec = np.zeros(self.DEFAULT_N_CLASSES, dtype=np.int32)\n",
    "                    one_hot_vec[tag_class_id] = 1\n",
    "                    self.tag_vector_map[tag] = tuple(one_hot_vec)\n",
    "                    self.tag_vector_map[tuple(one_hot_vec)] = tag\n",
    "                    tag_class_id += 1\n",
    "                    \n",
    "        print(\"raw_nd = \" + str(len(raw_data_train)))\n",
    "        \n",
    "        #Adding a None Tag\n",
    "        one_hot_vec = np.zeros(self.DEFAULT_N_CLASSES, dtype = np.int32)\n",
    "        one_hot_vec[tag_class_id] = 1\n",
    "        self.tag_vector_map['NONE'] = tuple(one_hot_vec)\n",
    "        self.tag_vector_map[tuple(one_hot_vec)] = 'NONE'\n",
    "        tag_class_id += 1\n",
    "        \n",
    "        self.n_sentences_all = len(raw_data_train)\n",
    "\n",
    "        # Find the maximum sequence length for Training data\n",
    "        self.max_sentence_len_train = 0\n",
    "        for seq in raw_data_train:\n",
    "            if len(seq[0]) > self.max_sentence_len_train:\n",
    "                self.max_sentence_len_train = len(seq[0])\n",
    "                \n",
    "                \n",
    "        ##########################  READ TEST DATA  ######################### \n",
    "        with open(test_file, 'r') as f_test:\n",
    "            \n",
    "            self.n_tag_classes = self.DEFAULT_N_CLASSES\n",
    "            tag_class_id = 0 \n",
    "            raw_data_test = []\n",
    "            raw_words_test = []\n",
    "            raw_tags_test = []        \n",
    "\n",
    "            # Process all lines in the file\n",
    "            for line in f_test:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    raw_data_test.append( (tuple(raw_words_test), tuple(raw_tags_test)))\n",
    "                    raw_words_test = []\n",
    "                    raw_tags_test = []\n",
    "                    continue\n",
    "                \n",
    "                word, tag = line.split('\\t') \n",
    "                \n",
    "                if tag not in self.tag_vector_map:\n",
    "                    print \"added\"\n",
    "                    one_hot_vec = np.zeros(self.DEFAULT_N_CLASSES, dtype=np.int32)\n",
    "                    one_hot_vec[tag_class_id] = 1\n",
    "                    self.tag_vector_map[tag] = tuple(one_hot_vec)\n",
    "                    self.tag_vector_map[tuple(one_hot_vec)] = tag\n",
    "                    tag_class_id += 1\n",
    "                \n",
    "                raw_words_test.append(word)\n",
    "                raw_tags_test.append(tag)\n",
    "                \n",
    "                                    \n",
    "        print(\"raw_nd = \" + str(len(raw_data_test)))\n",
    "        self.n_sentences_all = len(raw_data_test)\n",
    "\n",
    "        # Find the maximum sequence length for Test Data\n",
    "        self.max_sentence_len_test = 0\n",
    "        for seq in raw_data_test:\n",
    "            if len(seq[0]) > self.max_sentence_len_test:\n",
    "                self.max_sentence_len_test = len(seq[0])\n",
    "                \n",
    "        #Find the maximum sequence length in both training and Testing dataset\n",
    "        self.max_sentence_len = max(self.max_sentence_len_train, self.max_sentence_len_test)\n",
    "        \n",
    "        ############## Create Train Vectors################\n",
    "        self.all_X_train, self.all_Y_train = [], []\n",
    "        \n",
    "        self.unk_words = []\n",
    "        count = 0\n",
    "        for word_seq, tag_seq in raw_data_train:  \n",
    "            \n",
    "            elem_wordvecs, elem_tags = [], []            \n",
    "            for ix in range(len(word_seq)):\n",
    "                w = word_seq[ix]\n",
    "                t = tag_seq[ix]\n",
    "                w = w.lower()\n",
    "                if w in self.word_to_ix_map :\n",
    "                    count += 1\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[w])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "\n",
    "                elif \"UNK\" in self.word_to_ix_map :\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[\"UNK\"])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "                \n",
    "                else:\n",
    "                    w = \"UNK\"       \n",
    "                    new_wv = 2 * np.random.randn(self.DEFAULT_N_FEATURES) - 1 # sample from normal distribution\n",
    "                    norm_const = np.linalg.norm(new_wv)\n",
    "                    new_wv /= norm_const\n",
    "                    self.wordvecs = np.vstack((self.wordvecs, new_wv))\n",
    "                    self.word_to_ix_map[w] = self.wordvecs.shape[0] - 1\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[w])\n",
    "                    elem_tags.append(list(self.tag_vector_map[t]))\n",
    "\n",
    "            \n",
    "            # Pad the sequences for missing entries to make them all the same length\n",
    "            nil_X = zero_vec_pos\n",
    "            nil_Y = np.array(self.tag_vector_map['NONE'])\n",
    "            pad_length = self.max_sentence_len - len(elem_wordvecs)\n",
    "            self.all_X_train.append( ((pad_length)*[nil_X]) + elem_wordvecs)\n",
    "            self.all_Y_train.append( ((pad_length)*[nil_Y]) + elem_tags)\n",
    "\n",
    "        self.all_X_train = np.array(self.all_X_train)\n",
    "        self.all_Y_train = np.array(self.all_Y_train)\n",
    "        \n",
    "        ########################Create TEST Vectors##########################\n",
    "\n",
    "        self.all_X_test, self.all_Y_test = [], []\n",
    "        \n",
    "        for word_seq, tag_seq in raw_data_test:  \n",
    "            \n",
    "            elem_wordvecs, elem_tags = [], []            \n",
    "            for ix in range(len(word_seq)):\n",
    "                w = word_seq[ix]\n",
    "                t = tag_seq[ix]\n",
    "                w = w.lower()\n",
    "                if w in self.word_to_ix_map:\n",
    "                    count += 1\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[w])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "                    \n",
    "                elif \"UNK\" in self.word_to_ix_map :\n",
    "                    self.unk_words.append(w)\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[\"UNK\"])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "                    \n",
    "                else:\n",
    "                    self.unk_words.append(w)\n",
    "                    w = \"UNK\"\n",
    "                    self.word_to_ix_map[w] = self.wordvecs.shape[0] - 1\n",
    "                    elem_wordvecs.append(self.word_to_ix_map[w])\n",
    "                    elem_tags.append(self.tag_vector_map[t])\n",
    "                \n",
    "            # Pad the sequences for missing entries to make them all the same length\n",
    "            nil_X = zero_vec_pos\n",
    "            nil_Y = np.array(self.tag_vector_map['NONE'])\n",
    "            pad_length = self.max_sentence_len - len(elem_wordvecs)\n",
    "            self.all_X_test.append( ((pad_length)*[nil_X]) + elem_wordvecs)\n",
    "            self.all_Y_test.append( ((pad_length)*[nil_Y]) + elem_tags)\n",
    "\n",
    "        self.all_X_test = np.array(self.all_X_test)\n",
    "        self.all_Y_test = np.array(self.all_Y_test)\n",
    "        \n",
    "        print(\"UNK WORD COUNT \" + str(len(self.unk_words)))\n",
    "        print(\"Found WORDS COUNT \" + str(count))\n",
    "        print(\"TOTAL WORDS \" + str(count+len(self.unk_words)))\n",
    "        \n",
    "        \n",
    "\n",
    "        return (self.all_X_train, self.all_Y_train, self.all_X_test, self.all_Y_test, self.wordvecs)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 Create the Neural Network Model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NER_Model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NER_Model.py\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers.core import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import keras.backend as K\n",
    "import Data_Preparation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "class NER_Model:\n",
    "\n",
    "    def __init__ (self, reader):\n",
    "        \n",
    "        self.reader = reader\n",
    "        self.model = None\n",
    "        self.all_X_train, self.all_Y_train, self.all_X_test, self.all_Y_test, self.wordvecs = reader.get_data()\n",
    "        self.train_X = self.all_X_train\n",
    "        self.train_Y = self.all_Y_train\n",
    "        \n",
    "        self.test_X = self.all_X_test\n",
    "        self.test_Y = self.all_Y_test\n",
    "        \n",
    "    def load (self, filepath):\n",
    "        self.model = load_model(filepath)\n",
    "        \n",
    "    def save (self, filepath):\n",
    "        self.model.save(filepath)\n",
    "\n",
    "    def print_summary (self):\n",
    "        print(self.model.summary())\n",
    "        \n",
    "    def train (self, test_split = 0.2, epochs = 1, batch = 50, dropout = 0.2, reg_alpha = 0.0, units = 150, layers = 1):\n",
    "        \n",
    "        self.train_X = self.all_X_train\n",
    "        self.train_Y = self.all_Y_train\n",
    "        \n",
    "        self.test_X = self.all_X_test\n",
    "        self.test_Y = self.all_Y_test\n",
    "\n",
    "        print(\"Data Shapes\")\n",
    "        print(self.train_X.shape)\n",
    "        print(self.train_Y.shape)\n",
    "        print(self.test_X.shape)\n",
    "        print(self.test_Y.shape)\n",
    "        \n",
    "        dropout = 0.2\n",
    "\n",
    "        self.model = Sequential()        \n",
    "        self.model.add(Embedding(self.wordvecs.shape[0], self.wordvecs.shape[1], input_length = self.train_X.shape[1], \\\n",
    "                                 weights = [self.wordvecs], trainable = False))\n",
    "        \n",
    "        self.model.add(Bidirectional(LSTM(units, return_sequences = True)))\n",
    "        \n",
    "        self.model.add(Dropout(dropout))\n",
    "        \n",
    "        if layers > 1:\n",
    "            self.model.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "            self.model.add(Dropout(dropout))\n",
    "            \n",
    "        self.model.add(TimeDistributed(Dense(self.train_Y.shape[2], activation='softmax')))\n",
    "        \n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        print(self.model.summary())\n",
    "\n",
    "        self.model.fit(self.train_X, self.train_Y, epochs = epochs, batch_size = batch)\n",
    "        \n",
    "    def evaluate_1(self):\n",
    "        target = open(\"Pubmed_Output.txt\", 'w')\n",
    "        predicted_tags= []\n",
    "        test_data_tags = []\n",
    "        ind = 0\n",
    "        for x,y in zip(self.test_X, self.test_Y):\n",
    "            tags = self.model.predict(np.array([x]), batch_size=1)[0]\n",
    "            pred_tags = self.reader.decode_prediction_sequence(tags)\n",
    "            test_tags = self.reader.decode_prediction_sequence(y)\n",
    "            ind += 1\n",
    "            ### To see Progress ###\n",
    "            if ind%500 == 0: \n",
    "                print(\"Sentence\" + str(ind))\n",
    "\n",
    "            pred_tag_wo_none = []\n",
    "            test_tags_wo_none = []\n",
    "            \n",
    "            for index, test_tag in enumerate(test_tags):\n",
    "                if test_tag != \"NONE\":\n",
    "                    if pred_tags[index] == \"B-Chemical\":\n",
    "                        pred_tag_wo_none.append(\"B-Drug\")\n",
    "                    elif pred_tags[index] == \"I-Chemical\":\n",
    "                        pred_tag_wo_none.append(\"I-Drug\")\n",
    "                    elif pred_tags[index] == 'None':\n",
    "                        pred_tag_wo_none.append('O')\n",
    "                    else:\n",
    "                        pred_tag_wo_none.append(pred_tags[index])\n",
    "                        \n",
    "                    if test_tag == \"B-Chemical\":\n",
    "                        test_tags_wo_none.append(\"B-Drug\")\n",
    "                    elif test_tag == \"I-Chemical\":\n",
    "                        test_tags_wo_none.append(\"I-Drug\")\n",
    "                    else:                        \n",
    "                        test_tags_wo_none.append(test_tag)\n",
    "            \n",
    "            for wo in pred_tag_wo_none:\n",
    "                target.write(str(wo))\n",
    "                target.write(\"\\n\")\n",
    "            target.write(\"\\n\")\n",
    "            \n",
    "            for i,j in zip(pred_tags, test_tags):\n",
    "                if i != \"NONE\" and j != \"NONE\":\n",
    "                    test_data_tags.append(j)\n",
    "                    predicted_tags.append(i)\n",
    "\n",
    "        target.close()\n",
    "        \n",
    "        predicted_tags = np.array(predicted_tags)\n",
    "        test_data_tags = np.array(test_data_tags)\n",
    "        print(classification_report(test_data_tags, predicted_tags))\n",
    "\n",
    "        simple_conf_matrix = confusion_matrix(test_data_tags,predicted_tags)\n",
    "        all_tags = sorted(list(set(test_data_tags)))\n",
    "        conf_matrix = pd.DataFrame(columns = all_tags, index = all_tags)\n",
    "        for x,y in zip(simple_conf_matrix, all_tags):\n",
    "            conf_matrix[y] = x\n",
    "        conf_matrix = conf_matrix.transpose()\n",
    "        \n",
    "        return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 Train the network on the prepared data and obtain the predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Data_Preparation import Data_Preparation\n",
    "from NER_Model import NER_Model\n",
    "import cPickle as cp\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_FILEPATH = \"Drugs_and_Diseases//train_out.txt\"\n",
    "TEST_FILEPATH = \"Drugs_and_Diseases//test.txt\"\n",
    "\n",
    "vector_size = 50\n",
    "classes = 7 + 1\n",
    "seq_length = 613\n",
    "layer_arg = 2\n",
    "ep_arg = 10\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\\n\\nRunning on BIO-NLP data\\n\\n\\n\")\n",
    "    \n",
    "    # Read the data\n",
    "    print(\"Initializing data...\")\n",
    "    reader = Data_Preparation(classes, seq_length, TRAIN_FILEPATH, TEST_FILEPATH, vector_size)\n",
    "        \n",
    "    # Train the model\n",
    "    print(\"Training model... epochs = {0}, layers = {1}\".format(ep_arg, layer_arg))\n",
    "    nermodel = NER_Model(reader)\n",
    "    nermodel.train(epochs=ep_arg, layers=layer_arg)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    confusion_matrix = nermodel.evaluate_1()\n",
    "    print confusion_matrix\n",
    "\n",
    "    print(\"Done.\") \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 Generate the output of the model in the correct format for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file1 = open(\"Pubmed_Output.txt\")\n",
    "file2 = open(\"Drugs_and_Diseases//test.txt\")\n",
    "target = open(\"Drugs_and_Diseases//eval2.txt\", \"w\")\n",
    "\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "for line in file1:\n",
    "    list1.append(line)\n",
    "    \n",
    "for line in file2:\n",
    "    list2.append(line)\n",
    "    \n",
    "for ind, line in enumerate(list2):\n",
    "    x = line.split(\"\\t\")\n",
    "    if len(x) == 1:\n",
    "        target.write(\"\\n\")\n",
    "    else:\n",
    "        target.write(x[0])\n",
    "        target.write(\"\\t\")\n",
    "        if list1[ind] == \"NONE\":\n",
    "            target.write(\"O\")\n",
    "        else:\n",
    "            target.write(list1[ind])\n",
    "    ind += 1\n",
    "    \n",
    "file1.close()\n",
    "file2.close()\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Biomedical Entity Recognition Performance (Genaral)                                         \r\n",
      "                                                                                         number(recall/precision/f-score) \r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "|                  |          complete match         |       right boundary match      |       left boundary match       |\r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "|  Disease  (4083) | 2775 (67.96% / 70.36% / 69.14%) | 3211 (78.64% / 81.41% / 80.00%) | 2969 (72.72% / 75.28% / 73.98%) |\r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "|   Drug    (5392) | 4237 (78.58% / 75.43% / 76.97%) | 4299 (79.73% / 76.54% / 78.10%) | 4336 (80.42% / 77.19% / 78.77%) |\r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "|  [-ALL-]  (9475) | 7012 (74.01% / 73.34% / 73.67%) | 7510 (79.26% / 78.55% / 78.90%) | 7305 (77.10% / 76.40% / 76.75%) |\r\n",
      "+------------------+---------------------------------+---------------------------------+---------------------------------+\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./Drugs_and_Diseases/evalD_a_D.pl Drugs_and_Diseases/eval2.txt Drugs_and_Diseases/test.txt #with Embedding Layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BiomedicalEntityExtraction local",
   "language": "python",
   "name": "biomedicalentityextraction_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
